{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRE PROCESSING PHASE**\n",
    "\n",
    "In this phase we need to generate the final dataset (Image1,Image2,Caption). \n",
    "\n",
    "The steps to follow are this:\n",
    "1) Generate the embendings of the images with BLIP\n",
    "2) Compute the similarity in orther to obtain the pair (Image1,Image2) with a fine grained way.\n",
    "3) Use a captioning model for the generation of the two caption (Caption1,Caption2).\n",
    "4) Use an LLM for the creation of the \"Caption of Difference\".\n",
    "5) Concatenate all the things for the final creation of the dataset (Image1,Image2,Caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import lovely_tensors as lt\n",
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estrarre la path delle immagini da dentro la cartella imagesf2\n",
    "images_dir = \"D:\\\\COMPUTER VISION EXAM\\\\ComputeVisionProject\\\\images-hd\\\\\"\n",
    "image_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(images_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.jpeg'):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "\n",
    "len(image_paths)\n",
    "\n",
    "#creare un file csv con tutte le path\n",
    "with open('C:\\\\Users\\\\Utente\\\\Desktop\\\\PROJECTS\\\\Image_Difference_Captioning\\\\data\\\\processed\\\\image_paths.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"image_path\"])\n",
    "    for image_path in image_paths:\n",
    "        writer.writerow([image_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image path column into a list, skip the first element\n",
    "\n",
    "image_path_list = []\n",
    "with open('C:\\\\Users\\\\Utente\\\\Desktop\\\\PROJECTS\\\\Image_Difference_Captioning\\\\data\\\\processed\\\\image_paths.csv', mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    for lines in csvFile:\n",
    "        image_path_list.append(lines[0])\n",
    "\n",
    "image_path_list = image_path_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3640 [00:00<?, ?it/s]c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 3640/3640 [39:44<00:00,  1.53it/s] \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "image_features = {}\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Assicurati che il modello sia sulla GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(image_path_list), batch_size)):\n",
    "        batch_paths = image_path_list[i:i + batch_size]\n",
    "        batch = [Image.open(path) for path in batch_paths]\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Trasferisci gli input sulla GPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        features = model.get_image_features(**inputs)\n",
    "        image_features.update({path: feature for path, feature in zip(batch_paths, features)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file\n",
    "save_file(image_features,\"C:\\\\Users\\\\Utente\\\\Desktop\\\\PROJECTS\\\\Image_Difference_Captioning\\\\data\\\\processed\\\\image_features.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 98/116475 [00:06<1:59:15, 16.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# compute similarity between each vector and the entire tensor\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(image_features_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m----> 8\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# save in the csv file only the pair of images with the largest similarity for each image image_features_tensor[i]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     similarity[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_features = load_file(\"C:\\\\Users\\\\Utente\\\\Desktop\\\\PROJECTS\\\\Image_Difference_Captioning\\\\data\\\\processed\\\\image_features.safetensors\")\n",
    "image_features_tensor = torch.stack(list(image_features.values()))\n",
    "with open('C:\\\\Users\\\\Utente\\\\Desktop\\\\PROJECTS\\\\Image_Difference_Captioning\\\\data\\\\processed\\\\similarities_Pairs.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Image 1', 'Image 2', 'Similarity'])\n",
    "    # compute similarity between each vector and the entire tensor\n",
    "    for i in tqdm(range(image_features_tensor.shape[0])):\n",
    "        similarity = torch.nn.functional.cosine_similarity(image_features_tensor[i], image_features_tensor)\n",
    "        # save in the csv file only the pair of images with the largest similarity for each image image_features_tensor[i]\n",
    "        similarity[i] = 0\n",
    "        max_similarity = torch.max(similarity)\n",
    "        max_index = torch.argmax(similarity)\n",
    "        writer.writerow([image_path_list[i], image_path_list[max_index], max_similarity.item()])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
